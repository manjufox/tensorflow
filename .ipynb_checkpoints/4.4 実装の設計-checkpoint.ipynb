{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4.1.1 tensorflowの実装の流れ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(x):\n",
    "    #モデル全体の設定を行い、モデルの出力・予測結果を返す\n",
    "    \n",
    "def loss(y,t):\n",
    "    #モデルの誤差関数を定義し、誤差・損失を返す\n",
    "\n",
    "def training(loss):\n",
    "    #モデルの学習を行い、学習結果を返す\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    # 1. データの準備\n",
    "    # 2. モデル設定\n",
    "    y = inference(x)\n",
    "    loss = loss(y,t)\n",
    "    train_step = training(loss)\n",
    "    # 3. モデル学習\n",
    "    # 4. モデル評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference()の中身を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(x, keep_prob, n_in, n_hiddens, n_out):\n",
    "    #重み初期化\n",
    "    def weight_variable(shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "        return tf.Variable(initial)\n",
    "    #バイアス初期化\n",
    "    def bias_variable(shape):\n",
    "        initial = tf.zeros(shape)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    #入力層ー隠れ層、隠れ層ー隠れ層\n",
    "    for i, n_hidden in enumerate(n_hiddens):\n",
    "        if i == 0:\n",
    "            input = x\n",
    "            input_dim = n_in\n",
    "        else:\n",
    "            input = output\n",
    "            input_dim = n_hiddens[i-1]\n",
    "            \n",
    "        W = weight_variable([input_dim, n_hidden])\n",
    "        b = bias_variable([n_hidden])\n",
    "        \n",
    "        h = tf.nn.dropout(h, keep_prob)\n",
    "        \n",
    "    #隠れ層ー出力層\n",
    "    W_out = weight_variable([n_hiddens[-1], n_out])\n",
    "    b_out = bias_variable([n_out])\n",
    "    y = tf.nn.softmax(tf.matmul(output, W_out) + b_out)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(y,t):\n",
    "    cross_entropy = tf.reduce_mean( -tf.reduce_sum(t * tf.log(y), reduction_indices=[1]))\n",
    "    return cross_entropy\n",
    "\n",
    "def training(loss):\n",
    "    optimizer= tf.train.GradientDescentOptimizer(0.01)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4.1.2 Kerasによる実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もともとシンプルな実装が可能で以下の記述で実装可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_in = 784\n",
    "n_hiddens = [200, 200]\n",
    "n_out = 10\n",
    "activation = 'relu'\n",
    "p_keep = 0.5\n",
    "\n",
    "model = Sequential()\n",
    "for i, input_dim in enumerate(([n_in] + n_hiddens)[:-1]):\n",
    "    model.add(Dence(n_hiddens[i], input_dim=input_dim))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dropout(p_keep))\n",
    "    \n",
    "model.add(Dense(n_out))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlowにおけるモデルのクラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = DNN()\n",
    "model.fit(X_train, Y_train)\n",
    "model.evaluate(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "という風に書きたいとすると"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DNN(object):\n",
    "    def __init__(self):\n",
    "        #初期化処理\n",
    "    \n",
    "    #重み初期化\n",
    "    def weight_variable(self, shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    #バイアス初期化\n",
    "    def bias_variable(self, shape):\n",
    "        initial = tf.zeros(shape)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    #モデルの定義\n",
    "    def inference(self, x, keep_prob):\n",
    "        #モデルの定義\n",
    "        return y\n",
    "    \n",
    "    #誤差関数の定義\n",
    "    def loss(self, y, t):\n",
    "        cross_entropy = tf.reduce_mean( -tf.reduce_sum(t * tf.log(y), reduction_indices=[1]))\n",
    "        return cross_entropy\n",
    "    \n",
    "    #実行の定義\n",
    "    def training(self, loss):\n",
    "        optimizer= tf.train.GradientDescentOptimizer(0.01)\n",
    "        train_step = optimizer.minimize(loss)\n",
    "        return train_step\n",
    "    \n",
    "    #学習率の定義\n",
    "    def accuracy(self, y, t):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy\n",
    "    \n",
    "    #学習の定義\n",
    "    def fit(self, X_train, Y_train):\n",
    "        #学習の処理\n",
    "    \n",
    "    #評価の定義\n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        #評価の処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に書いてみると"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DNN(object):\n",
    "    #初期化処理\n",
    "    def __init__(self):\n",
    "        self.n_in = n_in\n",
    "        self.n_hiddens = n_hiddens\n",
    "        self.n_out = n_out\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        #...\n",
    "        self._x = None\n",
    "        self._t = None\n",
    "        self._keep_prob = None\n",
    "        self._history = {\n",
    "            'accuracy': [],\n",
    "            'loss': []\n",
    "        }\n",
    "        \n",
    "    #重み初期化\n",
    "    def weight_variable(self, shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    #バイアス初期化\n",
    "    def bias_variable(self, shape):\n",
    "        initial = tf.zeros(shape)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    #モデルの定義\n",
    "    def inference(self, x, keep_prob):\n",
    "        #入力層ー隠れ層、隠れ層ー隠れ層\n",
    "        for i, n_hidden in enumerate(self.n_hiddens):\n",
    "            if i == 0:\n",
    "                input = x\n",
    "                input_dim = self.n_in\n",
    "            else:\n",
    "                input = output\n",
    "                input_dim = self.n_hiddens[i-1]\n",
    "\n",
    "            self.weights.append(self.weight_variable([input_dim, n_hidden]))\n",
    "            self.biases.append(self.bias_variable([n_hidden]))\n",
    "            \n",
    "            h = tf.nn.relu(tf.matmul(input, self.weights[-1]) + self.biases[-1])\n",
    "            output = tf.nn.dropout(h, keep_prob)\n",
    "\n",
    "        #隠れ層ー出力層\n",
    "        self.weights.append(self.weight_variable([self.n_hiddens[-1], self.n_out]))\n",
    "        self.biases.append(self.bias_variable([self.n_out]))\n",
    "        \n",
    "        y = tf.nn.softmax(tf.matmul(output, self.weights[-1]) + self.biases[-1])\n",
    "        return y\n",
    "    \n",
    "    #誤差関数の定義\n",
    "    def loss(self, y, t):\n",
    "        cross_entropy = tf.reduce_mean( -tf.reduce_sum(t * tf.log(y), reduction_indices=[1]))\n",
    "        return cross_entropy\n",
    "    \n",
    "    #実行の定義\n",
    "    def training(self, loss):\n",
    "        optimizer= tf.train.GradientDescentOptimizer(0.01)\n",
    "        train_step = optimizer.minimize(loss)\n",
    "        return train_step\n",
    "    \n",
    "    #学習率の定義\n",
    "    def accuracy(self, y, t):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        return accuracy\n",
    "    \n",
    "    #学習の定義\n",
    "    def fit(self, X_train, Y_train, epochs=100, batch_size=100, p_keep=0.5, verbose=1):\n",
    "        x = tf.placeholder(tf.float32, shape=[None, self.n_in])\n",
    "        t = tf.placeholder(tf.float32, shape=[None, self.n_out])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        # evaluate()用に保持\n",
    "        self._x = x\n",
    "        self._t = t\n",
    "        self.keep_prob = keep_prob\n",
    "        \n",
    "        y = self.inference(x, keep_prob)\n",
    "        loss = self.loss(y, t)\n",
    "        train_step = self.training(loss)\n",
    "        accuracy = self.accuracy(y, t)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.Session()\n",
    "        sess.run(init)\n",
    "        \n",
    "        # evaluate()用に保持\n",
    "        self._sess = sess\n",
    "        \n",
    "        N_train = len(X_train)\n",
    "        n_batches = N_train // batch_size\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            X_, Y_ = shuffle(X_train, Y_train)\n",
    "            \n",
    "            for i in range(n_batches):\n",
    "                start = i * batch_size\n",
    "                end = start + batch_size\n",
    "                \n",
    "                sess.run(train_step, feed_dict={\n",
    "                    x:X_[start:end],\n",
    "                    t:Y_[start:end],\n",
    "                    keep_prob: p_keep\n",
    "                })\n",
    "            loss_ = loss.eval(session=sess, feed_dict={\n",
    "                x: X_train,\n",
    "                t: Y_train,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "            accuracy_ = accuracy.eval(session=sess, feed_dict={\n",
    "                x: X_train,\n",
    "                t: Y_train,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "            #値を記録しておく\n",
    "            self._history['loss'].append(loss_)\n",
    "            self._history['accuracy'].append(accuracy_)\n",
    "            \n",
    "            if verbose:\n",
    "                print('epoch:', epoch,\n",
    "                     ' loss:', loss,\n",
    "                     ' accuracy:', accuracy_)\n",
    "        return self._history\n",
    "    \n",
    "    #評価の定義\n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        return self.accuracy.eval(session=self._sess, feed_dict={\n",
    "            self._x: X_test,\n",
    "            self._t: Y_test,\n",
    "            self._keep_prob: 1.0\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを使えば"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = DNN(n_in=784, n_hiddens=[200, 200. 200], n_out=10)\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=50, batch_size=200, p_keep=0.5)\n",
    "\n",
    "accuracy = model.evaluate(X_test. Y_test)\n",
    "print('accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "と記述できるようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
